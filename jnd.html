<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html>

<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">



<title>ShenXuelin</title>

  <link rel="stylesheet" href="./OL_files/bootstrap.min.css">
  <link href="./OL_files/css" rel="stylesheet" type="text/css">
  <link href="./OL_files/style.css" rel="stylesheet" type="text/css">
  <script type="text/javascript" async="" src="./OL_files/ga.js.download"></script>
  <script type="text/javascript" src="./OL_files/hidebib.js.download"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-133713714-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-133713714-1');
</script>

</head>

<body>


<!-- Start main content -->

  <div class="container">
    <table width="900" border="0" align="center" cellpadding="20">
      </table><table width="90%" align="center" border="0" cellpadding="10">
        <tbody><tr>
          <td width="70%" valign="top">
            <p align="center">&nbsp;</p>

            <p align="center"><font size="6px">Just Noticeable Distortion Profile Inference: A Patch-level Structural Visibility Learning Approach
            </font><br></p>
            <p>Xuelin SHEN<sup>1</sup>, Zhangkai NI<sup>1</sup>, Wenhan YANG<sup>1</sup>, Shiqi WANG<sup>1</sup>, Xinfeng ZHANG<sup>2</sup>, Sam KWONG<sup>1</sup>  </p>
            <p style="MARGIN-TOP: 10pt; margin-bottom: 0pt;" align="center"><sup>1</sup>Department of Computer Science, City University of Hong Kong, Kowloon Tong, Hong Kong</p>
            <p style="MARGIN-TOP: 0pt; margin-bottom: 0pt;" align="center"><sup>2</sup>School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, China</p>

        </tr>
      </tbody></table>
  </div>

  <div class="container">
      <h2><p style="text-align:left; margin-bottom: 20pt; margin-top: 20pt;">VVC based JND dataset</p></h2><div class="bastract">
          <table>
              <tbody>
                  <tr>
                      <td width="1000">
                          <div class="example">
                              <img src="./OL_files/database.JPG" alt="Dummy Image" width="900" />
                          </div>
                      </td>
                  </tr>
              </tbody>
          </table>
          <br />

          <p style="text-align:justify"><font-family:"TimeNewRoman"> We establish a just noticeable distortion (JND) dataset based on the next generation video coding standard Versatile Video Coding (VVC). The dataset consists of 202 images which cover a wide range of content with resolution 1920×1080. Each image is encoded by VTM 5.0 intra coding with the quantization parameter (QP) ranging from 13 to 51. The details regarding dataset construction, subjective testing and data post-processing are described in this paper. Finally, the significance of the dataset towards future video coding re- search is envisioned. All source images as well as the testing data have been made available to the public.</font-family:"TimeNewRoman"></p>

          <a href="readme.txt"><h3>Readme and download link</h3></a>

      </div>

      <h2><p style="text-align:left; margin-bottom: 20pt; margin-top: 20pt;">Patch-wise structural visibility learning model</p></h2>
      <div class="method">
          <table>
              <tbody>
                  <tr>
                      <td width="1000">
                          <div class="example">
                              <img src="./OL_files/framework.jpg" alt="Dummy Image" width="900" />
                          </div>
                      </td>
                  </tr>
              </tbody>
          </table>
          <br />

          <p style="text-align:justify"><font-family:"TimeNewRoman"> we propose an effective approach to infer the just noticeable distortion (JND) profile based on patch- level structural visibility learning. Instead of pixel-level JND profile estimation, the image patch, which is regarded as the basic processing unit to better correlate with the human perception, can be further decomposed into three conceptually independent components for visibility estimation. In particular, to incorporate the structural degradation into the patch-level JND model, a deep learning-based structural degradation estimation model is trained to approximate the masking of structural visibility. In order to facilitate the learning process, a JND dataset is further established, including 202 pristine images and 7878 distorted images generated by advanced compression algorithms based on the upcoming Versatile Video Coding (VVC) standard. Extensive experimental results further show the superiority of the proposed approach over the state-of-the-art.</font-family:"TimeNewRoman"></p>

          <a href="https://github.com/ShenXuelin-CityU/PWJNDInfer"><h3>Code and pre-trained model</h3></a>

      </div>

      <h2><p style="text-align:left; margin-bottom: 20pt">Experimental results</p></h2>
      <div class="results">
          <table>
              <tbody>
                  <tr>
                      <td width="1000">
                          <div class="example">
                              <img src="./OL_files/results.JPG" alt="Dummy Image" width="900" />
                          </div>
                      </td>
                  </tr>
              </tbody>
          </table>
          <br />

          <p style="text-align:justify"><font-family:"TimeNewRoman"> The subjective results exhibited in Table V have demonstrated that the proposed JND profile is superior to the conventional JND models. In general, there are several reasons behind this. First, the patch decomposition method provides an analytical representation of visual information by independent visual factors, which means that all the corresponding visibility maskings are involved in JND profile generation. This leads to the proposed JND model being able to tolerate more distortions and achieve better subjective performance while maintaining the same PSNR comparing with the conventional models. Second, estimating the JND profile at the patch-level instead of pixel/sub-band level aligns with the HVS mechanism. Third, the learning based approach improves the model’s applicability which means the proposed model is able to achieve favorable performances in patches of different contents. These lead to accurate JND estimation.</font-family:"TimeNewRoman"></p>
      </div>
      <h3 style="text-align: left;">Refrence</h3>
      <p style="text-align:justify">
          <font-family:"TimeNewRoman">
              [4] A. Liu, W. Lin, M. Paul, C. Deng, and F. Zhang, “Just noticeable difference for images with decomposition model for separating edge and textured regions,” IEEE Transactions on Circuits and Systems for Video Technology, vol. 20, no. 11, pp. 1648–1652, 2010.
          </font-family:"TimeNewRoman">
      </p>
      <p style="text-align:justify">
          <font-family:"TimeNewRoman">
              [10] J. Wu, G. Shi, W. Lin, A. Liu, and F. Qi, “Just noticeable difference estimation for images with free-energy principle,” IEEE Transactions on Multimedia, vol. 15, no. 7, pp. 1705–1710, 2013.
          </font-family:"TimeNewRoman">
      </p>
      <p style="text-align:justify">
          <font-family:"TimeNewRoman">
              [11] J. Wu, L. Li, W. Dong, G. Shi, W. Lin, and Kuo. C-C Jay, “Enhanced just noticeable difference model for images with pattern complexity,”IEEE Transactions on Image Processing, vol. 26, no. 6, pp. 2682–2693,2017.
          </font-family:"TimeNewRoman">
      </p>
      <p style="text-align:justify">
          <font-family:"TimeNewRoman">
              [46] X. Yang, W. Lin, Z. Lu, E. On, and S. Yao, “Motion-compensated residue preprocessing in video coding based on just-noticeable-distortion profile,” IEEE Transactions on Circuits and Systems for Video Technology, vol. 15, no. 6, pp. 742–752, 2005.
          </font-family:"TimeNewRoman">
      </p>

      <h2></h2>
      <h3 style="text-align: left;">Citation</h3>
      <p style="text-align:justify"><font-family:"TimeNewRoman"> If our work is useful for your job, please kindly cite the following papers：</font-family:"TimeNewRoman"></p>
      <p style="text-align:justify"><font-family:"TimeNewRoman"> [1] X. Shen, Z. Ni, W. Yang, X. Zhang, S. Wang and S. Kwong, "A JND Dataset Based on VVC Compressed Images," 2020 IEEE International Conference on Multimedia & Expo Workshops (ICMEW), London, United Kingdom, 2020, pp. 1-6, doi: 10.1109/ICMEW46912.2020.9105955.</font-family:"TimeNewRoman"></p>
      <p style="text-align:justify">
          <font-family:"TimeNewRoman">
              [2] X. Shen, Z. Ni, W. Yang, X. Zhang, S. Wang and S. Kwong, "Just Noticeable Distortion Profile Inference: A Patch-level Structural Visibility Learning Approach," 2020 IEEE TIP.
          </font-family:"TimeNewRoman">
      </p>
      <h3 style="text-align: left;">Supplementary Material</h3>
      <p style="text-align: justify;">This document is mainly about the supplementary experiments which are required in the response letter. Details and corresponding results are provided.</p>
      <a href="Supplementary Material.pdf"><h3>Supplementary Material</h3></a>
      <h3 style="text-align: left;">Contact</h3>
      <p style="text-align: justify;">Thanks for your attention! If you have any suggestion or question, feel free to leave a message here or contact Xuelin Shen (xuelishen2-c@my.cityu.edu.hk).</p>


      <br />

  </div>



  <script
    src="https://cdnjs.cloudflare.com/ajax/libs/vue/2.1.6/vue.min.js">
  </script>

  <script src="./publications.js"></script>

  <script>
    new Vue({
      el: "#read_pub_record",
      data() {
        return {
          data: data,
        }
      }
    });
  </script>

  <!-- Publications.-->

  </div><br>



<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>




</body></html>
